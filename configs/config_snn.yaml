exp_name: 'trained_models/snn_experiment'  # Use forward slashes (works on all OS)

# RSNN parameters — Add these
net_type: 'snn'            # ✅ Key change to activate framework_snn.py
input_dim: 50              # ✅ Assuming 9x9 local FOV per agent (adjust as needed)
hidden_dim: 128            # Hidden size in SNN layer
num_actions: 5             # Assuming 5 possible movements (up, down, left, right, stay)

# You can remove or ignore CNN, GNN, encoder, policy MLP params

# Training
epochs: 50
tests_episodes: 100
device: cuda               # or 'cpu' if CUDA not available

# Simulation
board_size: [28,28]
map_shape: [9,9]
num_agents: 5
obstacles: 5
max_steps: 32
max_time: 32
sensing_range: 6           # Possibly 9x9 input

# Data Loading
min_time: 1
num_workers: 3
batch_size: 128
train:
    root_dir: 'dataset/5_8_28'   # ✅ Use forward slashes for cross-platform compatibility
    mode: 'train'
    min_time: 5
    max_time_dl: 25
    nb_agents: 5
    batch_size: 32
valid:
    root_dir: 'dataset/5_8_28'
    mode: 'valid'
    min_time: 5
    max_time_dl: 25
    nb_agents: 5
    batch_size: 8

# SNN neuron parameters - further optimized for spike propagation
lif_tau: 6.0           # Shorter time constant for faster response
lif_v_reset: 0.01       # Reset to zero after spike
lif_v_threshold: 0.09   # Even lower threshold to ensure spike propagation

adapt_tau: 200.0       # Longer adaptation for stability
adapt_alpha: 0.01      # Minimal adaptation to avoid suppressing spikes

# SNN temporal and attention parameters for framework_snn
snn_time_steps: 13      # Number of time steps for temporal context
tau: 9.0               # Membrane time constant (longer for better integration)
v_threshold: 0.08      # Lower threshold for reliable spiking
v_threshold_output: 0.15  # Very low threshold for output neurons
tau_output: 4.0        # Faster output dynamics
detach_reset: false     # Detach reset for gradient flow

# Input scaling - scale to encourage spiking
input_scale: 7.0       # Scale inputs to help reach firing threshold

# Synapse filter parameters
syn_tau: 5.0           # Increased synaptic time constant for better integration

# Transformer parameters - simplified
transformer_nhead: 4   # Number of attention heads (must divide embed_dim=128)
transformer_ff_dim: 128 # Reduced feedforward dimension to match feature size
transformer_layers: 1  # Simplified to 1 layer for faster learning
transformer2_layers: 1 # Single layer for second transformer

# SNN neuron scaling factors (multipliers on lif_tau) - more uniform
lif_scale_conv1: 1.0       # Base tau
lif_scale_conv2: 1.0       # Same tau for consistency
lif_scale_conv3: 1.0       # Same tau for consistency
lif_scale_feedback: 1.0    # Same tau for feedback
lif_scale_attention: 1.0   # Same tau for attention
lif_scale_conv4: 1.0       # Same tau for output

# Surrogate gradient parameters - higher alpha for sharper gradients
surrogate_alpha: 4.0        # Increased slope for better gradient flow

# SNN-specific parameters
#snn_time_steps: 7      # Number of time steps for temporal processing
#tau_output: 5.0        # Output layer neuron time constant  
#v_threshold_output: 0.1  # Lower threshold for output neurons
#detach_reset: false    # SpikingJelly parameter

# Recurrent connection parameters
recurrence_weight: 0.8 # How much previous hidden state influences current computation (0.0-1.0)
input_weight: 0.2     # How much current input influences computation (should sum to 1.0 with recurrence_weight)

# Graph SNN parameters for localized decision making
hesitation_weight: 0.2      # How much to inhibit quick decisions when uncertainty is high (0.0-1.0)
confidence_threshold: 0.7   # Threshold for confident decisions (0.0-1.0)
communication_weight: 0.3   # Weight for agent-to-agent communication influence

# Training hyperparameters - much lower learning rate for SNNs
learning_rate: 3e-4     # Reduced from 1e-1 to 1e-3 for stable SNN training
weight_decay: 2e-5      # Reduced weight decay
spike_reg_weight: 1e-4 # Much smaller spike regularization
spike_reg_coef: 1e-3


# Learning rate schedule and early stopping
lr_decay_factor: 0.6         # Gentler LR decay
lr_patience: 10              # More patience for SNN convergence
early_stop_patience: 25      # More patience for SNN learning
early_stop_threshold: 0.5    # Higher threshold (20% success rate)

# Spiking Transformer (Resformer) parameters
use_spiking_transformer: false    # Enable spiking transformer for better attention
transformer_nhead: 8              # Number of attention heads
transformer_ff_dim: 512           # Feedforward dimension in transformer

# Enhanced Architecture Parameters
use_transformer_encoder: true    # Enable transformer encoder for global spatial understanding
encoder_nhead: 8                 # Number of attention heads in transformer encoder
encoder_layers: 2                # Number of transformer encoder layers
encoder_dropout: 0.1             # Dropout in transformer encoder

# Dynamic Graph Parameters
proximity_threshold: 0.3         # Threshold for agent proximity-based connections
max_connections: 3               # Maximum connections per agent in dynamic graph
edge_learning_rate: 0.1          # Learning rate for edge weight adaptation

# Collision Loss Parameters
use_collision_loss: true         # Enable collision penalty in training
collision_loss_weight: 2.0       # Weight for collision penalty (higher = more penalty)
collision_loss_type: 'l2'        # Type of collision loss: 'l1', 'l2', or 'exponential'
collision_detection_radius: 1  # Radius for detecting collisions (1.0 = adjacent cells)
vertex_collision_weight: 1.1     # Weight for vertex collisions (same position)
edge_collision_weight: 1.1       # Weight for edge collisions (agents swapping positions)
future_collision_steps: 2        # Number of future steps to predict and penalize collisions