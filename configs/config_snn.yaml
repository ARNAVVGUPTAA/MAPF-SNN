exp_name: 'trained_models/snn_experiment'  # Use forward slashes (works on all OS)

# RSNN parameters — Add these
net_type: 'snn'            # ✅ Key change to activate framework_snn.py
input_dim: 50              # ✅ Assuming 9x9 local FOV per agent (adjust as needed)
hidden_dim: 128            # Hidden size in SNN layer
num_actions: 5             # Assuming 5 possible movements (up, down, left, right, stay)

# You can remove or ignore CNN, GNN, encoder, policy MLP params

# Training
epochs: 50
tests_episodes: 100
device: cuda               # or 'cpu' if CUDA not available

# Simulation
board_size: [28,28]
map_shape: [9,9]
num_agents: 5
obstacles: 5
max_steps: 32
max_time: 32
sensing_range: 6           # Possibly 9x9 input

# Data Loading
min_time: 1
num_workers: 3
batch_size: 128
train:
    root_dir: 'dataset/5_8_28'   # ✅ Use forward slashes for cross-platform compatibility
    mode: 'train'
    min_time: 5
    max_time_dl: 25
    nb_agents: 5
    batch_size: 32
valid:
    root_dir: 'dataset/5_8_28'
    mode: 'valid'
    min_time: 5
    max_time_dl: 25
    nb_agents: 5
    batch_size: 8

# SNN neuron parameters - further optimized for spike propagation
lif_tau: 5.0           # Shorter time constant for faster response
lif_v_reset: 0.0       # Reset to zero after spike
lif_v_threshold: 0.1   # Even lower threshold to ensure spike propagation

adapt_tau: 200.0       # Longer adaptation for stability
adapt_alpha: 0.01      # Minimal adaptation to avoid suppressing spikes

# SNN temporal and attention parameters for framework_snn
snn_time_steps: 4      # Number of time steps for temporal context
tau: 8.0               # Membrane time constant (longer for better integration)
v_threshold: 0.25      # Lower threshold for reliable spiking
v_threshold_output: 0.15  # Very low threshold for output neurons
tau_output: 4.0        # Faster output dynamics
detach_reset: true     # Detach reset for gradient flow

# Input scaling - scale to encourage spiking
input_scale: 5.0       # Scale inputs to help reach firing threshold

# Synapse filter parameters
syn_tau: 5.0           # Increased synaptic time constant for better integration

# Transformer parameters - simplified
transformer_nhead: 4   # Number of attention heads (must divide embed_dim=128)
transformer_ff_dim: 128 # Reduced feedforward dimension to match feature size
transformer_layers: 1  # Simplified to 1 layer for faster learning
transformer2_layers: 1 # Single layer for second transformer

# SNN neuron scaling factors (multipliers on lif_tau) - more uniform
lif_scale_conv1: 1.0       # Base tau
lif_scale_conv2: 1.0       # Same tau for consistency
lif_scale_conv3: 1.0       # Same tau for consistency
lif_scale_feedback: 1.0    # Same tau for feedback
lif_scale_attention: 1.0   # Same tau for attention
lif_scale_conv4: 1.0       # Same tau for output

# Surrogate gradient parameters - higher alpha for sharper gradients
surrogate_alpha: 4.0        # Increased slope for better gradient flow

# SNN-specific parameters
snn_time_steps: 5      # Number of time steps for temporal processing
tau_output: 5.0        # Output layer neuron time constant  
v_threshold_output: 0.15  # Lower threshold for output neurons
detach_reset: true     # SpikingJelly parameter

# Training hyperparameters - much lower learning rate for SNNs
learning_rate: 1e-3     # Reduced from 1e-1 to 1e-3 for stable SNN training
weight_decay: 1e-4      # Reduced weight decay
spike_reg_weight: 0.001 # Much smaller spike regularization

# Learning rate schedule and early stopping
lr_decay_factor: 0.8         # Gentler LR decay
lr_patience: 10              # More patience for SNN convergence
early_stop_patience: 25      # More patience for SNN learning
early_stop_threshold: 0.2    # Higher threshold (20% success rate)