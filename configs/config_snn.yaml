exp_name: 'trained_models/snn_experiment'  # Use forward slashes (works on all OS)

# RSNN parameters — Add these
net_type: 'snn'            # ✅ Key change to activate framework_snn.py
input_dim: 50              # ✅ Assuming 9x9 local FOV per agent (adjust as needed)
hidden_dim: 128            # Hidden size in SNN layer
num_actions: 5             # Assuming 5 possible movements (up, down, left, right, stay)

# You can remove or ignore CNN, GNN, encoder, policy MLP params

# Training
epochs: 50
tests_episodes: 100
device: cuda               # or 'cpu' if CUDA not available

# Simulation
board_size: [28,28]
map_shape: [9,9]
num_agents: 5
obstacles: 5
max_steps: 32
max_time: 32
sensing_range: 6           # Possibly 9x9 input

# Data Loading
min_time: 1
num_workers: 3
batch_size: 128
train:
    root_dir: 'dataset/5_8_28'   # ✅ Use forward slashes for cross-platform compatibility
    mode: 'train'
    min_time: 5
    max_time_dl: 25
    nb_agents: 5
    batch_size: 32
valid:
    root_dir: 'dataset/5_8_28'
    mode: 'valid'
    min_time: 5
    max_time_dl: 25
    nb_agents: 5
    batch_size: 8

# SNN neuron parameters
lif_tau: 7.0           # Membrane time constant (tau)
lif_v_reset: 0.2       # Reset voltage after spike
lif_v_threshold: 0.4   # Firing threshold

# SNN neuron scaling factors (multipliers on lif_tau)
lif_scale_conv1: 1.0       # conv1 uses base lif_tau
lif_scale_conv2: 0.75      # conv2 multiplier
lif_scale_conv3: 0.6       # conv3 multiplier
lif_scale_feedback: 0.5    # feedback LIF multiplier
lif_scale_attention: 0.5   # recurrent attention LIF multiplier
lif_scale_conv4: 0.75      # conv4 LIF multiplier

# Surrogate gradient parameters
surrogate_alpha: 4.0        # slope alpha for Sigmoid surrogate function

# Convolution kernel sizes (if wanting to parameterize)
# conv1_kernel: 7
# conv2_kernel: 5
# conv3_kernel: 3
# conv4_kernel: 5
# feedback_kernel: 1

# Residual block channels and kernels
# res_block_channels: 256
# res_block_kernel: 3

# Synapse Filter
syn_tau: 5.0               # Synaptic time constant

# Transformer parameters
transformer_nhead: 5      # Number of attention heads
transformer_ff_dim: 512    # Feedforward dimension in Transformer layers
transformer_layers: 4      # Number of layers in first transformer
transformer2_layers: 3     # Number of layers in second transformer

# Training hyperparameters
learning_rate: 1e-2     # Learning rate for optimizer
weight_decay: 1e-3      # Weight decay for optimizer
spike_reg_weight: 0.5   # L1 regularization weight for spikes

# Learning rate schedule and early stopping
lr_decay_factor: 0.5         # Factor to reduce LR when plateauing
lr_patience: 5               # Number of epochs with no improvement before LR decay
early_stop_patience: 15      # Number of epochs to wait for min success rate
early_stop_threshold: 0.1    # Minimum success rate required within patience